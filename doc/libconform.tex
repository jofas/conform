\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[page]{appendix}
\usepackage{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}

\def\version{\texttt{v0.1.0}}
\def\libconform{\texttt{libconform}}

\title{\libconform{} \version: a Python library for
       conformal prediction}

\author{\name Jonas Fa{\ss}bender
        \email jonas@fassbender.dev}

\ShortHeadings{\libconform{} \version{}}{Fa{\ss}bender}
\firstpageno{1}

\begin{document}

\maketitle

%\begin{abstract}%
%\end{abstract}

%\begin{keywords}
%\end{keywords}


\section{Introduction}

This paper introduces the Python library \libconform,
implementing concepts defined in \citet{alrw}, namely the
conformal prediction framework and Venn prediction for
reliable machine learning and predicting with certainty.
These algorithms address a weakness of more traditional
machine learning algorithms which produce only bare
predictions, without their confidence in them/the
probability of the prediction, therefore providing no
measure of likelihood, desirable and even necessary in many
real-world application domains.

The conformal prediction framework is composed of
variations of the conformal prediction algorithm,
first described in
\citet{vovk_et_al_1999, saunders_et_al_1999}.
A conformal predictor provides a measurement of confidence
in its predictions.
A Venn predictor, on the other hand, provides a
multi-probabilistic measurement, making it a probabilistic
predictor.
Below in the text, Venn predictors are included if only
``conformal prediction framework'' is written, except
stated otherwise.

The conformal prediction framework is applied successfully
in many real-world domains, for example face recognition,
medical diagnostic and prognostic and network traffic
classification \citep[see][Part 3]{cprml}.

It is build on traditional machine learning algorithms, the
so called underlying algorithms
\citep[see][]{papadopoulos_et_al_2007}, which makes Python
the first choice for implementation, since its machine
learning libraries are top of the class, still evolving and
improving due to the commitment of a great community of
developers and researchers.

\libconform's aim is to provide an easy to use, but very
extensible API for the conformal prediction framework, so
developers can use their preferred implementations for the
underlying algorithm and can leverage the library, even in
this early stage.
\libconform{} \version{} is \textbf{not} yet stable; there
are still features missing and the API is very likely to
change and improve.
The library is licensed under the MIT-license and its
source code can be downloaded from
\url{https://github.com/jofas/conform}.

This paper combines \libconform{}'s documentation with an
outline of the implemented algorithms.
At the end of each chapter there are notes on the
implementation containing general information about the
library, descriptions of the internal workings and the API
and possible changes in future versions.

Appendix~\ref{appendix:a} provides an overview over
\libconform's API and Appendix~\ref{appendix:b} contains
examples on how to use the library.

\section{Conformal predictors}

Like stated in the introduction, this chapter will only
outline conformal prediction (CP). For more details see
\citet{alrw}.

CP---like the name suggests---determines the label(s) of an
incoming observation based on how well it/they conform(s)
with previous observed examples. Conformal prediction can
be used either in the online or the offline---or
batch---learning setting.
The offline learning setting, compared to online learning,
weakens the validity and efficiency---described below in
this chapter---of the classifier in favor of computational
efficiency \citep[see][Chapter 4]{alrw}.

Let $\Lbag z_1,\dots,z_n \Rbag$ be a bag, also called
multiset%
\footnote{It is typical in machine learning to denote this
as a data set, even though examples do not have to
be unique, making the so called set a multiset. A multiset
is not a list, since the ordering of the elements is not
important.},
of examples, where each example $z_i \in \textbf{Z}$ is a
tuple $(x_i,y_i); x_i \in \textbf{X}, y_i \in \textbf{Y}$.
\textbf{X} is called the observation space and \textbf{Y}
the label space. For this time \textbf{Y} is considered
finite, making the task of prediction a classification
task, rather than regression, which will be considered in
Chapter~\ref{subsec:rrcm}.

A conformal predictor can be defined as a confidence
predictor $\Gamma$. For this an input $\epsilon \in (0,1)$,
the significance level is needed. $1 - \epsilon$ is called
the confidence level.
A conformal predictor $\Gamma^\epsilon$ in the online
setting is conservatively valid under the exchangeability
assumption, which means, as long as exchangeability holds,
it makes errors at a frequency of $\epsilon$ or less. For
more on that refer to \citet[][Chapters 1--4, 7]{alrw}.

CP, in its original setting, produces nested prediction
sets. Rather than returning a single label as its
prediction, it returns a set of elements
$\textbf{Y}^\prime \in 2^{\textbf{Y}}$; $2^{\textbf{Y}}$
being the set of all subsets of $\textbf{Y}$, including the
empty set.
The prediction sets are called nested, because, for
$\epsilon_1 \geq \epsilon_2$, the prediction set of
$\Gamma^{\epsilon_1}$ is a subset of $\Gamma^{\epsilon_2}$
\citep[see][Chapter 2]{alrw}.

The efficiency of a classifier is the frequency with which
it classifies new observations with a single label.

In order to predict the label of a new observation
$x_{n+1}$, set $z_{n+1}:=(x_{n+1}, y)$, for each
$y \in \textbf{Y}$ and check how $z_{n+1}$ conforms with
the examples of our bag $\Lbag z_1,\dots,z_n \Rbag$.

This is done with a nonconformity measure
$A_{n+1}:\textbf{Z}^n \times \textbf{Z} \rightarrow
\mathbb{R}$. First, $z_{n+1}$ is added to the bag, then
$A_{n+1}$ assigns a numerical score to each example in
$z_i$:
\begin{align}
  \alpha_i = A_{n+1}(\Lbag z_1,\dots,z_{i-1},z_{i+1},
             \dots,z_{n+1} \Rbag, z_i).
\label{eq:a0}
\end{align}
One can see in this equation that $z_i$ is removed from the
bag. It is also possible to compute $\alpha_i$ with $z_i$
in the bag, which means for
$A_{n+1}:\textbf{Z}^{n+1} \times \textbf{Z} \rightarrow
\mathbb{R}$ the score is computed as:
\begin{align}
  \alpha_i = A_{n+1}(\Lbag z_1,\dots,z_{n+1} \Rbag, z_i).
\label{eq:a1}
\end{align}
Which one is preferable is case-dependent
\citep[see][Chapter 4.2.2]{shafer_et_al_2008}.

$\alpha_i$ is called nonconformity score.
The nonconformity score can now be used to compute the
p-value for $z_{n+1}$, which is the fraction of examples
from the bag which are at least as nonconforming as
$z_{n+1}$:
\begin{align}
  \frac{|\{i=1,\dots,n+1: \alpha_i \geq \alpha_{n+1}\}|}
       {n + 1}.
\label{eq:p0}
\end{align}
Another way to determine the p-value is through smoothing,
in which case the nonconformity scores equal to
$\alpha_{n+1}$ are multiplied by a random value
$\tau_{n+1}$:
\begin{align}
  \frac{|\{i=1,\dots,n+1: \alpha_i > \alpha_{n+1}\}|
    + \tau_{n+1} |\{i=1,\dots,n+1:\alpha_i=\alpha_{n+1}\}|}
       {n + 1}.
\label{eq:p1}
\end{align}

A conformal predictor using the smoothed p-value is called
a smoothed conformal predictor and is exactly valid under
exchangeability in the online setting, which means it makes
errors at a rate exactly $\epsilon$
\citep[see][Chapter 2]{alrw}.
If the p-value of $z_{n+1}$ is larger than $\epsilon$, $y$
is added to the prediction set.

\begin{algorithm}
  \caption{: Conformal predictor $\Gamma^\epsilon
    (\Lbag z_1,\dots,z_n \Rbag, x_{n+1})$}
  \label{alg:cp}

  \begin{algorithmic}[1]
    \FORALL{$y \in \textbf{Y}$}
      \STATE{set $z_{n+1} := (x_{n+1}, y)$
             and add it to the bag}
      \FORALL{$i=1,\dots,n+1$}
        \STATE{compute $\alpha_i$ with (\ref{eq:a0})
               or (\ref{eq:a1})}
      \ENDFOR
      \STATE{set $p_y$ with (\ref{eq:p0}) or (\ref{eq:p1})}
      \IF{$p_y > \epsilon$}
        \STATE{add $p_y$ to prediction set}
      \ENDIF
    \ENDFOR
    \RETURN{prediction set}
  \end{algorithmic}
\end{algorithm}
\textcolor{white}{bad workaround}\\

\noindent
\textbf{Notes on the implementation:}
\libconform{} provides the \texttt{CP} class for creating
conformal prediction classifiers. \libconform{}'s
classifier classes provide quite equal APIs, only with
minor variations.
The API of the classifier classes is comparable to major
machine learning libraries like sklearn or keras
\citep[see][]{sklearn_api, keras}.

It is common in machine learning to split the learning task
in two distinct operations, first a
training---or fit---operation on a bag of examples
and then a predict operation on new observations.
\libconform{}'s classifier classes follow this style,
providing a \texttt{train} and a \texttt{predict} method.

While this split in training and predicting is common for
inductive classifiers, which first derive a prediction
rule, or decision surface, from the training bag and then
predict unseen examples inductively based on that rule,
it is not really the way CP works. CP was designed to be
transductive, not inductive, which means rather than
to generate a prediction rule, it uses all previous seen
examples to classify a new observation, making the training
step unnecessary (see Algorithm~\ref{alg:cp}).
While the transductive setting is more elegant than the
inductive setting, it is computationally very expensive and
not feasible for larger bags of examples and for underlying
algorithms---discussed in Chapter~\ref{subsec:ncs}---which
have a computationally complex training phase
\citep[see][Chapter 1]{papadopoulos_et_al_2007,alrw}.

\libconform{}'s aim is to be---one day---ready for
production, where, for some application domains, the time
complexity of predicting a new observation is crucial,
while the time complexity of the training phase
is---in a certain range---not as important.
Therefore \libconform{}'s \texttt{CP} class tries to
minimize the time complexity of its \texttt{predict}
method. Instead of adding $z_{n+1}$ to the bag and then
computing $\alpha_i$ for each example in the bag during
prediction, it computes $\alpha_1,\dots,\alpha_n$ during
training and only computes $\alpha_{n+1}$ in
\texttt{predict} (see Algorithm~\ref{alg:cp}, lines 3--5).
Therefore---not adding $z_{n+1}$ to the bag---it
currently computes the nonconformity scores based on
$A_n$ instead of $A_{n+1}$.

Arguably \texttt{CP} does not implement the conformal
prediction algorithm in its original form, being currently
rather a special case of inductive conformal prediction,
where the calibration set is equal to the whole bag of
examples previously witnessed, instead of a subset (see
Chapter~\ref{sec:icp}).
It is possible that \texttt{CP} will change to being the
implementation of the original conformal prediction
algorithm in a future version, or simply providing an extra
method for the computationally more demanding original
online learning setting \citet[see][Chapter 2]{alrw}.

\texttt{CP} takes an instance of a nonconformity measure
$A$ and a sequence of $\epsilon_1,\dots,\epsilon_{g}$
as its arguments during initialization, therefore being the
implementation of
$\Gamma^{\epsilon_1},\dots,\Gamma^{\epsilon_g}$.

It also provides to extra utility methods for validation,
\texttt{score} and \texttt{score\_online}, which generate
metrics for the conformal predictors
$\Gamma^{\epsilon_1},\dots,\Gamma^{\epsilon_g}$.
The most important of those metrics are the error rates
$Err_1,\dots,Err_{g}$.
It the error rate $Err_i \leq \epsilon_i$ over the bag of
examples provided to \texttt{score}/\texttt{score\_online}
than $\Gamma^{\epsilon_i}$ was valid on the bag.

\texttt{score\_online} adds an example, after it was
predicted, to the training bag and calls \texttt{train},
using $\Gamma^{\epsilon_1},\dots,\Gamma^{\epsilon_g}$ in
the online learning setting.

\texttt{CP} also provides a method for another setting of
conformal prediction, this one not based on a significance
level $\epsilon$: \texttt{predict\_best}.
\texttt{predict\_best} always returns a single label, the
one with the highest p-value and optionally also its
significance level. The significance level is the second
highest p-value, since a label is added to the prediction
set---in the original setting---if its p-value is greater
than $\epsilon$ \citep[see][]{papadopoulos_et_al_2007}.

\subsection{Nonconformity measures based on underlying
            algorithms}
\label{subsec:ncs}

Previously nonconformity measures were only described as
any function
$A: \textbf{Z}^* \times \textbf{Z} \rightarrow \mathbb{R}$,
$\textbf{Z}^*$ being any possible bag of examples from
$\textbf{Z}$. This chapter will make a more concrete
description on what nonconformity measures are and how they
use underlying traditional machine learning algorithms.

Let $D:\textbf{Z}^* \times \textbf{X} \rightarrow
\hat{\textbf{Y}}$ be a traditional machine learning
algorithm. $\hat{\textbf{Y}}$ must not be equal to
$\textbf{Y}$. Furthermore there exists a discrepancy
measure $\Delta: \textbf{Y} \times \hat{\textbf{Y}}
\rightarrow \mathbb{R}$.
For a concrete bag $\Lbag z_1,\dots,z_n \Rbag$,
$D_{\Lbag z_1,\dots,z_n \Rbag}$ would be the instance of
$D$ trained on the bag, generating a decision surface based
on it. $D_{\Lbag z_1,\dots,z_n \Rbag}(x)$ would return the
label $\hat{y}$ for $x$. Now we can define the
nonconformity score $\alpha$ for $z := (x, y)$ from the
nonconformity measure $A_n$ as:
\begin{align*}
  \alpha=A_n(\Lbag z_1,\dots,z_n \Rbag, z)=
  \Delta\big(y,D_{\Lbag z_1,\dots,z_n \Rbag}(x)\big),
\end{align*}
or rather with removed example for any $\alpha_i$,
$i = 1,\dots,n$:
\begin{align*}
  \alpha_i = A_n(\Lbag z_1,\dots,z_{i-1},z_{i+1},\dots,z_n
  \Rbag, z_i) = \Delta\big(y_i,D_{\Lbag z_1,\dots,z_{i-1},
  z_{i+1},\dots,z_n \Rbag}(x_i)\big).
\end{align*}

Especially the second equation can be computationally very
complex since it requires to refit $D$ for each
$i = 1,\dots,n$. In general it is not very natural to
use a inductive decision surface $D$ within the
transductive framework of CP.

A popular nonconformity measure is based on the nearest
neighbor method \citep[see][]{alrw,shafer_et_al_2008,cprml,
  smirnov_et_al_2009}.
The general description for the $k$-nearest neighbor method
can be found in \citet{smirnov_et_al_2009}, the other
articles/books describe the nonconformity measure based on
the 1-nearest neighbor method for $z:=(x,y)$ as:
\begin{align*}
  A_n(\Lbag z_1,\dots,z_n \Rbag, z) =
  \frac{\text{min}_{i=1,\dots,n:y_i = y} d(x,x_i)}
       {\text{min}_{i=1,\dots,n:y_i \neq y} d(x,x_i)},
\end{align*}
$d$ being a distance measure, for example the Euclidean
distance. It should be noted that $A_n$ based on the
1-nearest neighbor method for
$A_n(\Lbag z_1,\dots,z_n \Rbag, z_i), i=1,\dots,n$ requires
the removal of $z_i$ from the bag, since otherwise the
smallest distance for $y_i = y_j,j=1,\dots,n$ would always
be 0 resulting in worthless nonconformity scores.

The more general nonconformity measure based on the
$k$-nearest neighbor method can be described as:
\begin{align*}
  A_n(\Lbag z_1,\dots,z_n \Rbag, z)=\frac{d_k^y}{d_k^{-y}},
\end{align*}
$d_k$ being the sum of the $k$ smallest distances to $x$,
$-y$ being all the examples where
$y \neq y_i, i = 1,\dots,n$.
\\\\

\noindent
\textbf{Notes on the implementation:}
\libconform{} tries again to be as extensible as possible,
providing a way for developers to define their own
nonconformity measures. For nonconformity measures
\libconform{} provides a module \texttt{ncs} containing
predefined nonconformity measures and a base class for
inheritance called \texttt{NCSBase}.

Predefined are currently the $k$-nearest neighbor method,
one based on a decision tree \citep[see][Chapter 4]{alrw}
and one based on neural networks
\citep[see][Chapter 4]{papadopoulos_et_al_2007,alrw}.
The $k$-nearest neighbor method and the decision tree
are based on the sklearn library
\citep[see][]{sklearn_api}.

Nonconformity measures are classes inheriting from
\texttt{NCSBase} and have to provide an interface with
three methods: \texttt{train}, \texttt{scores} and
\texttt{score}.

\texttt{train}$: \textbf{X}^n \times \textbf{Y}^n$ is for
fitting the underlying algorithm $D$ to a bag of examples.

\texttt{scores}$: \textbf{X}^m \times \textbf{Y}^m \times
bool \rightarrow \mathbb{R}^m$ returning the scores for a
bag of examples. The $bool$ value provided as a parameter
tells the nonconformity measure if the bag is equal to the
bag provided to \texttt{train}, making it possible to
implement (\ref{eq:a0}), rather than (\ref{eq:a1}).
The CP-implementation passes the same bag to \texttt{train}
and \texttt{scores}, while the inductive conformal
prediction implementation (see Chapter~\ref{sec:icp})
passes another bag---the so called calibration set---as a
parameter to \texttt{scores}.

\texttt{score}$: \textbf{X} \times
\textbf{Y}^{|\textbf{Y}|} \rightarrow
\mathbb{R}^{|\textbf{Y}|}$ is for returning the scores of
an example $x$ and each $y \in \textbf{Y}$ combined as
$z := (x, y)$.

\subsection{Conformal predictor for regression: ridge
            regression confidence machine}
\label{subsec:rrcm}

The ridge regression confidence machine algorithm is
described in
\citet[Chapter 2.3]{nouretdinov_et_al_2001, alrw}.
In this chapter, unlike in the previous and following
chapters, $\textbf{Y}$ will be $\mathbb{R}$, making the
prediction a regression problem instead of classification.

Algorithm~\ref{alg:cp} is not feasible for regression,
since $\textbf{Y}$ is now infinite and we would need to
test for each $y \in \textbf{Y}$ if it is in the prediction
set or not. Instead the ridge regression confidence machine
(RRCM) algorithm offers a different approach, returning
prediction intervals instead of prediction sets.

Even though RRCM has ridge regression in its name, it can
be used with other underlying algorithms, like nearest
neighbor regression. For more on ridge regression and its
special case linear regression refer to e.g.
\citet[Chapter 3]{elem_stat}.

Let $\Lbag z_1,\dots,z_n \Rbag$ be our bag of examples,
let $z_{n+1} := (x_{n+1}, y)$ be the observation we want to
predict and let $D_{\Lbag z_1,\dots,z_{n+1} \Rbag}$ be an
underlying regression algorithm.
Previously nonconformity scores were treated as constants,
now we treat them as functions, since $y$ is now an unknown
variable: $\alpha_i(y) = |a_i + b_i y|, i=1,\dots,n+1$.
$a_i$ and $b_i$ are provided by the underlying regression
algorithm. Each $b_i$ is always positive, if not $a_i$ and
$b_i$ are multiplied with $-1$.

Now we can compute the set of $y$'s which p-values are
exceeding a significance level $\epsilon$.
Let $S_i = \{y: |a_i + b_i y| \geq |a_{n+1} + b_{n+1} y|\},
i=1,\dots,n$. Each $S_i$ looks like:
\begin{align*}
S_i =
  \begin{cases}
    [u_i,v_i]
      &\quad \text{if } b_{n+1} > b_i
      \\
    (-\infty, u_i] \cup [v_i, \infty)
      &\quad \text{if } b_{n+1} < b_i
      \\
    [u_i, \infty)
      &\quad \text{if } b_{n+1} = b_i > 0
      \text{ and } a_{n+1} < a_i
      \\
    (-\infty, u_i]
      &\quad \text{if } b_{n+1} = b_i > 0
      \text{ and } a_{n+1} > a_i
      \\
    \mathbb{R}
      &\quad \text{if } b_{n+1} = b_i = 0
      \text{ and } |a_{n+1}| \leq |a_i|
      \\
    \emptyset
      &\quad \text{if } b_{n+1} = b_i = 0
      \text{ and } |a_{n+1}| > |a_i|
  \end{cases},
\end{align*}
so each $S_i$ is either an interval, a point (a special
interval), the union of two rays, a ray, the real line or
empty.
$u_i$ and $v_i$ are either the minimum/maximum of
$-\frac{a_i - a_{n+1}}{b_i - b_{n+1}}$ and
$-\frac{a_i + a_{n+1}}{b_i + b_{n+1}}$,
if $b_{n+1} \neq b_{i}$ or
$u_i = v_i = -\frac{a_i + a_n}{2b_i}$, if
$b_{n+1} = b_i > 0$
The p-value can only change at $u_i$ or $v_i$.
Therefore all $u_i$ and $v_i$ are sorted in ascending
order generating the sequence $s_1,\dots,s_m$ plus two more
$s$-values, $s_0=-\infty$, $s_{m+1}=\infty$.
The p-value is constant on any interval
$(s_i,s_{i+1}),i=0,\dots,m$ from the sorted set
\citep[see][]{nouretdinov_et_al_2001}.

After that $N$ and $M$ are computed from the sequence.
$N_j, j=0,\dots,m$ for the interval $(s_j,s_{j+1})$ is the
count of $S_i: (s_j,s_{j+1}) \subseteq S_i, i=1,\dots,n$.
$M_j, j=1,\dots,m$, on the other hand, does the same count
only for single $s_j$: $S_i: s_j \in S_i, i=1,\dots,n$.

For a given significance level $\epsilon$ the prediction
interval is the union of intervals from $N$ and points from
$M$ for which $\frac{N_j}{n+1} > \epsilon$ or
$\frac{M_j}{n+1} > \epsilon$, respectively
\citep[see][Chapter 2.3]{alrw}.

In \citet{nouretdinov_et_al_2001} it is stated that there
could be holes in the prediction interval, which means the
the RRCM would return more than a single prediction
interval.
According to the authors these holes rarely show in
empirical tests.
The authors therefore remark that the RRCM can just remove
those holes---therefore returning a single interval---by
simply returning the convex hull of the prediction
intervals.
\\\\

\noindent
\textbf{Notes on the implementation:}
The ridge regression confidence machine is implemented as
a class \texttt{RRCM}. It provides the same API as
\texttt{CP}. It implements a computationally less complex
prediction method than the one described above. While the
RRCM described above runs at $\mathcal{O}(n^2)$,
\texttt{RRCM} takes only $\mathcal{O}(n \log n)$, because it
does not compute $N$ and $M$ directly but instead
\begin{align*}
N_{j}^\prime =
  \begin{cases}
    N_j - N_{j-1} &\quad \text{if } j=0,\dots,m \\
    0             &\quad \text{if } j=-1
  \end{cases}
\end{align*}
and
\begin{align*}
M_{j}^\prime =
  \begin{cases}
    M_j - M_{j-1} &\quad \text{if } j=1,\dots,m \\
    0             &\quad \text{if } j=0
  \end{cases},
\end{align*}
which takes only $\mathcal{O}(n)$, making the sorting the
$u_i$'s and $v_i$'s the most complex task
\citep[see][Chapter 2.3]{alrw}.

The \texttt{RRCM} implementation takes a flag during
its initialization for dealing with the holes described
above, so developers can choose if they want possibly more
than one prediction interval or the convex hull.

\texttt{RRCM} is based on underlying regression algorithms
providing it with its $a_i$ and $b_i$. Currently the
library provides one of these regression algorithms, based
on the $k$-nearest neighbor method.
$a_i$ is the difference between $y_i$ and the average of
the labels of its $k$-nearest neighbors. $y_{n+1}$ is set
to 0, therefore the $k$-nearest neighbor method returns the
negated average of the labels of the $k$-nearest neighbors
of $x_{n+1}$ as $a_{n+1}$.
For $b_i, i=1,\dots,n$ it returns 0, for $b_{n+1}$ it
returns 1.

For developing underlying regression scorers there exists
a base class for inheritance called
\texttt{NCSBaseRegressor}.
It provides a comparable API to \texttt{NCSBase}---the
base class for nonconformity measures---described
in the previous chapter.
Like \texttt{NCSBase} the API contains a \texttt{train}
method for training the underlying algorithm.
Instead of \texttt{scores} and \texttt{score} it has
\texttt{coeffs} and \texttt{coeffs\_n}. The first returns
for a bag two vectors of coefficients $a_i$ and $b_i$ for
each element in the bag. \texttt{coeffs\_n} returns
the coefficients for the observation that needs to be
predicted, in this chapter $z_{n+1} := (x_{n+1},y)$.

\section{Inductive conformal predictors}
\label{sec:icp}

Suppose we have an underlying inductive machine learning
algorithm $D$ as our nonconformity measure and a bag
$\Lbag z_1,\dots,z_n \Rbag$ of examples. If we want to use
$D$ as the nonconformity measure we need to fit it to our
bag: $D_{\Lbag z_1,\dots,z_n \Rbag}$. For some $D$ this can
be a quite time consuming task and in general is not a very
aesthetic thing to do in our transductive setting from the
previous chapter, since---if we would want to predict a
new observation $x_{n+1}$---we would need to refit $D$ for
each $y \in \textbf{Y}$, because we compute our
nonconformity scores adding $z_{n+1} := (x_{n+1},y)$ to
the bag and refitting $D$ with it
(see Algorithm~\ref{alg:cp}, lines 2--5).
Even worse, if we would use (\ref{eq:a0}) instead of
(\ref{eq:a1}) we would need to refit $D$ for each bag
$\Lbag z_1,\dots,z_{i-1},z_{i+1},\dots,z_{n+1} \Rbag,
i=1,\dots,n$.

There exists a natural derivation from the transductive
setting of conformal prediction to the inductive setting
called inductive conformal prediction (ICP).
ICP works more natural with nonconformity measures relying
on inductive machine learning algorithms as the underlying
algorithm \citep[see][Chapter 4]{alrw}.

ICP is computationally less complex than CP, to the cost of
the classifier's validity and efficiency
\citep[see][Chapter 4]{alrw}.

Suppose, again, we have our bag of examples
$\Lbag z_1,\dots,z_n \Rbag$. ICP now splits this bag at a
point $m < n$ into two bags, the so called training set
$\Lbag z_1,\dots,z_m \Rbag$ and the calibration set
$\Lbag z_{m+1},\dots,z_n \Rbag$.

With the training set the underlying algorithm is trained
generating $D_{\Lbag z_1,\dots,z_m \Rbag}$. For each
example in the calibration set the nonconformity score
$\alpha_i$ gets computed:
\begin{align}
  \label{eq:a_icp}
  \alpha_i=\Delta(y_i,D_{\Lbag z_1,\dots,z_m \Rbag}(x_i)),
  i=m+1,\dots,n.
\end{align}

Now, for an incoming example $x_g, g \in [n+1,\infty)$ set
$z_{g} := (x_g, y)$ for each $y \in \textbf{Y}$ and compute
the nonconformity score $\alpha_g$ like (\ref{eq:a_icp}).
The p-value of $z_{g}$ is:
\begin{align*}
  \frac{|\{i = m+1,\dots,n: \alpha_i \geq \alpha_g\}|}
       {n-m+1}
\end{align*}
\citep[see][]{papadopoulos_et_al_2007}.

The huge costs of fitting $D$ repetitively are now reduced
to fitting $D$ only once.
More elaborate update cycles---called teaching
schedules---where $m$ is changing after certain events
and how they impact the validity of the classifier can be
found in \citet[Chapters 4.3, 4.4]{alrw}.
\\\\

\noindent
\textbf{Notes on the implementation:}
\texttt{ICP} is the class implemention inductive conformal
prediction. It provides the same API as \texttt{CP},
except \texttt{score\_online}. It has an additional method
\texttt{calibrate} for generating the nonconformity scores
for a the calibration set.

It works with the same nonconformity measures (instances
of classes inheriting from \texttt{NCSBase}) as does
\texttt{CP}.

Currently the nonconformity scores from the calibration set
are saved internally as a vector. In future releases this
will change to an optimized data structure for searching,
e.g.\ a red-black tree.

\section{Mondrian or conditional (inductive) conformal
         predictors}

The property of validity under the exchangeablity
assumption can be further optimized with mondrian or
conditional (inductive) conformal prediction (MCP).
In \citet[Chapter 4.5]{alrw} this form of conformal
prediction is called mondrian, in \citet[Chapter 2]{cprml}
it is called conditional.

An example from \citet[Chapter 4.5]{alrw} makes it clear
why the stronger form of validity provided by MCP can be
important for some real-world application domains.
The authors tested a 1-nearest neighbor based smoothed
conformal predictor with the significance level
$\epsilon=0.05$ on the USPS data set.
The USPS data set contains 9298 images of handwritten
digits.
The observations are a $16 \times 16$ matrix where each
cell is in the interval of $(-1,1)$ and the labels
obviously are 0 to 9 \citep[see][]{lecun_et_al_1989}.
They found out, that while overall the validity (the error
frequency was nearly equal to $\epsilon=0.05$) held, the
smoothed conformal predictor had an error rate of $11.7\%$
on examples with the label ``5''.
The smoothed conformal predictor masked its bad performance
on examples with the label ``5'' simply by predicting other
labels with an error rate of less than $\epsilon = 0.05$,
e.g.\ for the label ``0'' the error rate was below $0.01$
\citep[see][Chapter 4.5]{alrw}.

The idea of MCP is to partition the examples into a
discrete and finite set $\textbf{K}$ of categories
$k_i \in \textbf{K}$ and to achieve conditional validity in
each category.
For the partitioning a measurable function called a
taxonomy is used.
In \citet[Chapter 4.5]{alrw} the taxonomy is called
mondrian taxonomy and is defined as:
\begin{align*}
  \kappa: \mathbb{N} \times \textbf{Z} \rightarrow
  \textbf{K},
\end{align*}
in \citet[Chapter 2]{cprml} the taxonomy is called a
$n$-taxonomy:
\begin{align*}
  K_n: \textbf{Z}^n \rightarrow \textbf{K}^n.
\end{align*}

The mondrian taxonomy $\kappa$ takes the index $i$ of an
example $z_i$ from a sequence $z_1,\dots,z_n$ and $z_i$ as
its input and maps it to a category while the $n$-taxonomy
$K_n$ takes a sequence of examples with a size of $n$ and
maps it to a sequence of categories with size $n$.
$K_n$ is more flexible than $\kappa$ since it is possible
to make the decision which category an example from the
sequence should be in based on the other examples from the
sequence.

The $K$-conditional p-value for an example $z_{n+1}$ and
a bag $\Lbag z_1,\dots,z_n \Rbag$ is now defined for
$i=1,\dots,n+1$ as:
\begin{align}
  \label{eq:mp0}
  \frac{|\{i:K_i = K_{n+1} \text{ \& } \alpha_i
        \geq \alpha_{n+1}\}|}
  {|\{i: K_i = K_{n+1}\}|}.
\end{align}
The smoothed version would be:
\begin{align}
  \label{eq:mp1}
  \frac{|\{i:K_i = K_{n+1} \text{ \& } \alpha_i >
        \alpha_{n+1}\}| + \tau_{n+1}
        |\{i:K_i = K_{n+1} \text{ \& } \alpha_i =
        \alpha_{n+1}\}|}
  {|\{i: K_i = K_{n+1}\}|}.
\end{align}

(\ref{eq:mp0}) and (\ref{eq:mp1}) are the same for $\kappa$
if $K$ is substituted with $\kappa$.

A MCP classifier is category-wise valid under the
exchangeability assumption \citep[see][]{alrw, cprml}.
\\\\

\noindent
\textbf{Notes on the implementation:}
There is no direct implementation for MCP, \libconform{}
rather leverages the fact that CP and ICP are just a
special form of mondrian (inductive) conformal prediction,
where $|\textbf{K}| = 1$, which means all examples are in
the same category.
\texttt{CP} and \texttt{ICP} can take an argument during
initialization called \texttt{mondrian\_taxonomy}.
Currently \texttt{mondrian\_taxonomy} is a function---or
a Python callable rather---which takes one example as its
input and returns the category, basically a 1-taxonomy
$K_1$ where the single example can only be looked at
without context.

In practice a single example often is more information than
really needed.
Often just the label of the example is important, making
the MCP based on this $\textbf{K}$ a label conditional
(inductive) conformal predictor
\citep[see][Chapter 2]{cprml}.

\texttt{mondrian\_taxonomoy} will change in a future
version to $K_n$ for more flexibility.

\citet[Chapter 4.5]{alrw} defines mondrian nonconformity
measures
\begin{align*}
  A: \textbf{K}^* \times \textbf{Z}^* \times
  \textbf{K} \times \textbf{Z} \rightarrow \mathbb{R},
\end{align*}
which add the category to each example in order to compute
the nonconformity scores.
Currently \libconform{} does not have an API for mondrian
nonconformity measures, which could change in
future releases.

\section{Probabilistic prediction: Venn predictors}

\section{Meta-conformal predictors}

\section{Conclusion}

\renewcommand{\appendixpagename}{}
\begin{appendices}
  \section*{Appendices}

  \section{API reference}
  \label{appendix:a}

  \section{Examples}
  \label{appendix:b}

\end{appendices}

\bibliography{libconform.bib}

\end{document}
