\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage{fontawesome}
\usepackage[page]{appendix}
\usepackage{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{marginnote}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}

\def\version{\texttt{v0.1.0}}
\def\libconform{\texttt{libconform}}
\def\info{\textcolor{blue}{\faInfo}}
\def\alert{\textcolor{red}{\faExclamation}}

\title{\libconform{} \version: a Python library for
       conformal prediction}

\author{\name Jonas Fa{\ss}bender
        \email jonas@fassbender.dev}

\ShortHeadings{\libconform{} \version{}}{Fa{\ss}bender}
\firstpageno{1}

\begin{document}

\maketitle

%\begin{abstract}%
%\end{abstract}

%\begin{keywords}
%\end{keywords}


\section{Introduction}

This paper introduces the Python library \libconform,
implementing concepts defined in \citet{alrw}, namely the
conformal prediction framework and Venn prediction for
reliable machine learning.
These algorithms address a weakness of more traditional
machine learning algorithms which produce only bare
predictions, without their confidence in them/the
probability of the prediction, therefore providing no
measure of likelihood, desirable and even necessary in many
real-world application domains.

The conformal prediction framework is composed of
variations of the conformal prediction algorithm (CP),
first described in
\citet{vovk_et_al_1999, saunders_et_al_1999}.
A conformal predictor provides a measurement of confidence
in its predictions.
A Venn predictor, on the other hand, provides a
multi-probabilistic measurement, making it a probabilistic
predictor.
Below in the text, Venn predictors are included if only
``conformal prediction framework'' is written, except
stated otherwise.

The conformal prediction framework is applied successfully
in many real-world domains, for example face recognition,
medical diagnostic and prognostic and network traffic
classification \citep[see][part 3]{cprml}.

It is build on traditional machine learning algorithms, the
so called underlying algorithms
\citep[see][]{papadopoulos_et_al_2007}, which makes Python
the first choice for implementation, since its machine
learning libraries are top of the class, still evolving and
improving due to the commitment of a great community of
developers and researchers.

\libconform's aim is to provide an easy to use, but very
extensible API for the conformal prediction framework, so
developers can use their preferred implementations for the
underlying algorithm and can leverage the library, even in
this early stage.
\libconform{} \version{} is \textbf{not} yet stable; there
are still features missing and the API is very likely to
change and improve.
The library is licensed under the MIT-license and its
source code can be downloaded from
\url{https://github.com/jofas/conform}.

This paper combines \libconform{}'s documentation with a
outline of the implemented algorithms.
Paragraphs marked with \info{} contain general information
about the library and descriptions of the
internal workings, while paragraphs marked
with \alert{} describe changes in future versions.

Appendix~\ref{appendix:a} provides an overview over
\libconform's API and Appendix~\ref{appendix:b} contains
examples on how to use the library.

%\marginnote{\info{}}[2.7em]

\section{Conformal predictors}

Like stated in the introduction, this chapter will only
outline conformal prediction (CP). For more details see
\citet{alrw}.

CP---like the name suggests---determines the label(s) of an
incoming observation based on how well it conforms with
previous observed examples.
Let $\Lbag z_1,\dots,z_n \Rbag$ be a bag, also called
multiset%
\footnote{It is typical in machine learning to denote this
as the training set, even though examples do not have to
be unique, making the so called set a multiset. A multiset
is not a list, since the ordering of the elements is not
important.},
of examples, where each example $z_i \in \textbf{Z}$ is a
tuple $(x_i,y_i); x_i \in \textbf{X}, y_i \in \textbf{Y}$.
\textbf{X} is called the observation space and \textbf{Y}
the label space. For this time \textbf{Y} is considered
finite, making the task of prediction a classification
task, rather than regression, which will be considered in
chapter~\ref{subsec:rrcm}.

A conformal predictor can be defined as a confidence
predictor $\Gamma$. For this an input $\epsilon \in (0,1)$,
the significance level is needed. $1 - \epsilon$ is called
the confidence level. A conformal predictor
$\Gamma^\epsilon$ is conservatively valid under the
exchangeability assumption, which means, as long as
exchangeability holds, it makes errors at
a frequency of $\epsilon$ or less. For more on that refer
to \citet[][chapters 1,2,7]{alrw}.

CP, in its original setting, produces nested prediction
sets. Rather than returning a single label as its
prediction, it returns a set of elements
$\textbf{Y}^\prime \in 2^{\textbf{Y}}$, $2^{\textbf{Y}}$
being the set of all subsets of $\textbf{Y}$, including the
empty set.
The prediction sets are called nested, because, for
$\epsilon_1 \geq \epsilon_2$, the prediction of
$\Gamma^{\epsilon_1}$ is a subset of $\Gamma^{\epsilon_2}$
\citep[see][chapter 2]{alrw}.

In order to predict the label of a new observation
$x_{n+1}$, set $z_{n+1}:=(x_{n+1}, y)$, for each
$y \in \textbf{Y}$ and check how $z_{n+1}$ conforms with
the examples of our bag $\Lbag z_1,\dots,z_n \Rbag$.

This is done with a nonconformity measure
$A_{n+1}:\textbf{Z}^n \times \textbf{Z} \rightarrow
\mathbb{R}$. First, $z_{n+1}$ is added to the bag, then
$A_{n+1}$ assigns a numerical score to each example in
$z_i$:
\begin{align}
  \alpha_i = A_{n+1}(\Lbag z_1,\dots,z_{i-1},z_{i+1},
             \dots,z_{n+1} \Rbag, z_i).
\label{eq:a0}
\end{align}
One can see in this equation that $z_i$ is removed from the
bag. It is also possible to compute $\alpha_i$ with $z_i$
in the bag, which means for
$A_{n+1}:\textbf{Z}^{n+1} \times \textbf{Z} \rightarrow
\mathbb{R}$ the score is computed as:
\begin{align}
  \alpha_i = A_{n+1}(\Lbag z_1,\dots,z_{n+1} \Rbag, z_i).
\label{eq:a1}
\end{align}
Which one is preferable is case-dependent
\citep[see][chapter 4.2.2]{shafer_et_al_2008}.

$\alpha_i$ is called nonconformity score.
The nonconformity score can now be used to compute the
p-value for $z_{n+1}$, which is the fraction of examples
from the bag which are at least as nonconforming as
$z_{n+1}$:
\begin{align}
  \frac{|\{i=1,\dots,n+1: \alpha_i \geq \alpha_{n+1}\}|}
       {n + 1}.
\label{eq:p0}
\end{align}
Another way to determine the p-value is through smoothing,
in which case the nonconformity scores equal to
$\alpha_{n+1}$ are multiplied by a random value
$\tau_{n+1}$:
\begin{align}
  \frac{|\{i=1,\dots,n+1: \alpha_i > \alpha_{n+1}\}|
    + \tau_{n+1} |\{i=1,\dots,n+1:\alpha_i=\alpha_{n+1}\}|}
       {n + 1}
\label{eq:p1}
\end{align}
A conformal predictor using the smoothed p-value is called
a smoothed conformal predictor and is exactly valid under
exchangeability, which means it makes errors at a rate
exactly $\epsilon$ \citep[see][chapter 2]{alrw}.
If the p-value of $z_{n+1}$ is larger than $\epsilon$, $y$
is added to the prediction set.

\begin{algorithm}
  \caption{: Conformal predictor $\Gamma^\epsilon
    (\Lbag z_1,\dots,z_n \Rbag, x_{n+1})$}

  \begin{algorithmic}[1]
    \FORALL{$y \in \textbf{Y}$}
      \STATE{set $z_{n+1} := (x_{n+1}, y)$
             and add it to the bag}
      \FORALL{$i=1,\dots,n+1$}
        \STATE{compute $\alpha_i$ with (\ref{eq:a0})
               or (\ref{eq:a1})}
      \ENDFOR
      \STATE{set $p_y$ with (\ref{eq:p0}) or (\ref{eq:p1})}
      \IF{$p_y > \epsilon$}
        \STATE{add $p_y$ to prediction set}
      \ENDIF
    \ENDFOR
    \RETURN{prediction set}
  \end{algorithmic}
\end{algorithm}

% INFO on how implemented

\subsection{Nonconformity measures based on underlying
            algorithms}

% D(x)

% here info on how NCScores impl in libconform

\subsection{Conformal predictor for regression: ridge
            regression confidence machine}
\label{subsec:rrcm}

\section{Inductive conformal predictors}

\section{Mondrian (inductive) conformal predictors}

\section{Probabilistic prediction: Venn predictors}

\section{Meta-conformal predictors}

\section{Conclusion}

\renewcommand{\appendixpagename}{}
\begin{appendices}
  \section*{Appendices}

  \section{API reference}
  \label{appendix:a}

  \section{Examples}
  \label{appendix:b}

\end{appendices}

\bibliography{libconform.bib}

\end{document}
